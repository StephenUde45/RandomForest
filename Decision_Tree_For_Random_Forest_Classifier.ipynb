{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature # Sets the feature attribute of the node\n",
    "        self.threshold = threshold # Represents the threshold falue for the feature\n",
    "        self.left = left # Refers to the left child node of the current node\n",
    "        self.right = right # Refers to the right child node of the current node\n",
    "        self.value = value # Intended to store the prediction value for a leaf node\n",
    "    \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None # Returns a boolean value which checks whether the value attribute of the node is not None\n",
    "    \n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None): # Definess the constructor of the decision tree class\n",
    "        self.min_samples_split = min_samples_split # This controls the minimum number of samples required to split an internal node\n",
    "        self.max_depth = max_depth # Is the length of the longest path from the root to a leaf\n",
    "        self.n_features = n_features # used to specify the number of features to conisder for looking for the best split at each node\n",
    "        self.root = None # Intended to store the root node of the decision tree once it is built\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.n_features = x.shape[1 ] if not self.n_features else min(x.shape[1], self.n_features) # Ensures n_features is set correctly\n",
    "        self.root = self._grow_tree(x, y) # Initializes the root attribute by calling the internal method _grow_tree(x, y)\n",
    "\n",
    "    def _grow_tree(self, x, y, depth=0):\n",
    "        n_samples, n_feats = x.shape # Extracts the number of samples and features from x data matrix\n",
    "        n_labels = len(np.unique(y)) # Calculates the number of unique classes or labels in array y\n",
    "\n",
    "        if(depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split): # Checks the current depth, if there is only one label in array y, and the number of samples\n",
    "            leaf_value = self._most_common_label(y) \n",
    "            return Node(value=leaf_value)\n",
    "        \n",
    "        feat_idx = np.random.choice(n_feats, self.n_features, replace=False) # Each integer represents an index that is randomly selected and that number can't be used again\n",
    "        \n",
    "        best_feature, best_thresh = self._best_split(x, y, feat_idx) # Determines the best feature and threshold for splitting the dataset. \n",
    "\n",
    "        left_idxs, right_idxs = self._split(x[:, best_feature], best_thresh) # 2D structure that slices the array to get the values for the best features across all samples\n",
    "        left = self._grow_tree(x[left_idxs, :], y[left_idxs], depth+1) # Part of the recursive tree building process that creates the left tree branch \n",
    "        right = self._grow_tree(x[right_idxs, :], y[right_idxs], depth+1) # Part of the recursive tree building process that created the right tree branch\n",
    "        return Node(best_feature, best_thresh, left, right) \n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y) # Counts hashable objects \n",
    "        value = counter.most_common(1)[0][0] # Indexes into a tuple to get the first value. Used to determine the majority class at a leaf node\n",
    "        return value\n",
    "\n",
    "    def _best_split(self, x, y, feat_idxs):\n",
    "        best_gain = -1 # Ensures any actual gain whether positive or negative will be larger\n",
    "        split_index, split_threshold = None, None \n",
    "\n",
    "        for feat_idx in feat_idxs: # Iterates over each feature index\n",
    "            x_column = x[:, feat_idx] # Extracts the entire column from the dataset x\n",
    "            thresholds = np.unique(x_column) # Finds all unique values in the selected feature column \n",
    "\n",
    "            for thr in thresholds: # Iterates over each unique threshold value in the current feature column  \n",
    "                gain = self._information_gain( y, x_column, thr) # calculates the information gain\n",
    "\n",
    "                if gain > best_gain: # Checks if the information gain is greater than the best gain found so far\n",
    "                    best_gain = gain \n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thr\n",
    "                    \n",
    "        return split_idx, split_threshold\n",
    "    \n",
    "    def _information_gain(self, y, x_column, threshold):\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        left_idxs, right_idxs = self._split(x_column, threshold) # Divides the indices of the dataset into two groups \n",
    "\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0: # Checks if right or left indicies are empty \n",
    "               return 0 \n",
    "        \n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs) # the number of samples that fall into the left and right group after the split\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs]) # The target variable in the left and right indicies are calculated here\n",
    "        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r # The weighted average of the two indicies are calculated here\n",
    "\n",
    "        information_gain = parent_entropy - child_entropy \n",
    "        return information_gain\n",
    "\n",
    "        \n",
    "    def _split(self, x_column, split_threshold):\n",
    "        # The .flatten() is used to convert the 2D array into a 1D array\n",
    "        left_idxs = np.argwhere(x_column <= split_threshold).flatten() # Finds the indicies of all samples in x_samples. Results in a 2D array where each row is a list containing the index of a satisfying element. \n",
    "        right_idxs = np.argwhere(x_column > split_threshold).flatten() # Finds the indicies of all samples in x_samples. Results in a 2D array where each row is a list containing the index of a satisfying element. \n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        y_int = y.astype(int) # Coverts y into an array of integers\n",
    "        hist = np.bincount(y_int) # Computes a histogram of non negative integers\n",
    "        ps = hist / len(y_int) # Calculates the probability of each unique label\n",
    "        return -np.sum([p * np.log2(p) for p in ps if p>0]) # Computes the entropy using the Shannon entropy formula\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X]) # Iterates over each sample x in the array x\n",
    "    \n",
    "    def _traverse_tree(self, x, node): \n",
    "        if node.is_leaf_node(): # Checks if the current node is a leaf node\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] <= node.threshold: \n",
    "            return self._traverse_tree(x, node.left) \n",
    "        return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9590643274853801\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data = datasets.load_breast_cancer() # Calls in dataset from sklearn\n",
    "x, y = data.data, data.target # Unpacks the data into the variables x and y. X containing input variables and y containing target variables for labels\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.3, random_state = 1234\n",
    ") # Sets the training set to be 70 percent and 30 oercent will be used as the test set. Random state sets the random number generator to a number specified \n",
    "\n",
    "clf = DecisionTree()\n",
    "clf.fit(x_train, y_train) # Trains the decision tree using the training data\n",
    "predictions = clf.predict(x_test) # Used to classify the data \n",
    "\n",
    "def accuracy(y_test, y_pred):\n",
    "    return np.sum(y_test == y_pred) / len(y_test) # Produces the accuracy of the algorithm(.10 means 10 percent and so on) \n",
    "    \n",
    "\n",
    "acc = accuracy(y_test, predictions)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, n_trees = 10, max_depth = 10, min_samples_split = 2, n_features = None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.n_features = n_features\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees): # Creates and trains individual decision trees\n",
    "            tree = DecisionTree(max_depth = self.max_depth, \n",
    "                         min_samples_split = self.min_samples_split, \n",
    "                         n_features = self.n_features) # A new instance is created for each interation of the loop\n",
    "            \n",
    "            x_sample, y_sample = self._bootstrap_samples(x, y)\n",
    "            tree.fit(x_sample, y_sample) # This trains the decision tree on the sampled data\n",
    "            self.trees.append(tree) # Appends to the list self.trees after the decision tree is trained\n",
    "\n",
    "    def _bootstrap_samples(self, x, y):\n",
    "        n_samples = x.shape[0] # Gets the number of samples in thne dataset\n",
    "        idxs = np.random.choice(n_samples, n_samples, replace=True) # Generates a random sample of indices with replacement\n",
    "        return x[idxs], y[idxs]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        predictions = np.array([tree.predict(x) for tree in self.trees]) # Uses a list complrehension to  call the predict method on each decision tree in the random forest\n",
    "        tree_preds = np.swapaxes(predictions, 0, 1) # Used to swap the axes pf the predictions array\n",
    "        predictions = np.array([self._most_common_label(pred) for pred in tree_preds]) # Determines the most common prediction for each sample across all the trees\n",
    "        return predictions \n",
    "    \n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y) \n",
    "        most_common = counter.most_common(1)[0][0] \n",
    "        return most_common\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9532163742690059\n"
     ]
    }
   ],
   "source": [
    "data = datasets.load_breast_cancer()\n",
    "x, y = data.data, data.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.3, random_state = 2345\n",
    ")\n",
    "\n",
    "clf = RandomForest()\n",
    "clf.fit(x_train, y_train)\n",
    "predictions = clf.predict(x_test)\n",
    "\n",
    "def accuracy(y_test, y_pred):\n",
    "    return np.sum(y_test == y_pred) / len(y_test)\n",
    "    \n",
    "\n",
    "acc = accuracy(y_test, predictions)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
